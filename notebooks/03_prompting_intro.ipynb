{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ee5027-b428-4abc-bb78-bb178d82f40a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89beca3-c09e-4b88-83d8-1ddd3d91cba3",
   "metadata": {},
   "source": [
    "For this notebook, we have two goals.\n",
    "\n",
    "1) get familiar with invoking different LLMs using Langchain.\n",
    "2) Explore the response of LLM's for the query rewriting use case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d70e68-61ac-496d-905e-1a5e9df040a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df26609e-b7db-4378-b186-aeab31d061f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "faf1e6de-a122-4855-be75-41c270702fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI, ChatGooglePalm\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import Replicate\n",
    "\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "import tiktoken\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline \n",
    "from langchain.llms import HuggingFaceHub\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950b6937-e9b7-40e1-9581-7a4d96f62f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc129c7b-fe99-4b14-8209-ecddfb3c64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_debug(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0e9119-9e5a-4076-9825-3261dc90c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']=\"python\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28741cba-d748-47f4-94a0-7d7422e09cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../.env\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d18d6-1143-44db-8694-5306c9eb6b59",
   "metadata": {},
   "source": [
    "to save time and cost, we make sure to save the LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93b02610-fdad-40e9-9096-8189cc7b69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678ca05-02b9-4639-a1fd-28a8c349a45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b54a8d7e-34ac-4af6-8fc0-d4adbe7548f7",
   "metadata": {},
   "source": [
    "## Use Case: Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1af5f-433c-4f14-bfb3-e55b59b728b4",
   "metadata": {},
   "source": [
    "### LLM: OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598844a9-e480-4b79-ab91-6c7c219058ff",
   "metadata": {},
   "source": [
    "All the availalbe OpenAI models can be found [here](https://platform.openai.com/docs/models/continuous-model-upgrades) \n",
    "\n",
    "Here we are using Gpt4, that supports 8k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef84a728-3aaa-481c-a457-3a8148095c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai= ChatOpenAI(model=\"gpt-4\")\n",
    "#llm_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32373585-c922-4ffb-8e3b-b88c07054ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73f82b8-296d-411f-82f4-9126ef900123",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0da23c64-a4c6-4e39-b82c-e51ec92f81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can Barack Obama have a conversation with George Washington?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a871a4c3-2045-48e7-899c-5b0ab2a47f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer: '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45351f92-f63d-489b-9fa8-02053b36e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_gpt4 = LLMChain(llm=llm_openai, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c06b2-e46e-4d8b-b8d5-f45471896ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5d9267-1816-49e3-b847-2b94f0b58f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Can Barack Obama have a conversation with George Washington?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [2.12s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"No, Barack Obama cannot have a conversation with George Washington because George Washington died in 1799, long before Obama was born.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"No, Barack Obama cannot have a conversation with George Washington because George Washington died in 1799, long before Obama was born.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 26,\n",
      "      \"prompt_tokens\": 22,\n",
      "      \"total_tokens\": 48\n",
      "    },\n",
      "    \"model_name\": \"gpt-4\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [2.12s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"No, Barack Obama cannot have a conversation with George Washington because George Washington died in 1799, long before Obama was born.\"\n",
      "}\n",
      "Tokens Used: 48\n",
      "\tPrompt Tokens: 22\n",
      "\tCompletion Tokens: 26\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0022199999999999998\n",
      "No, Barack Obama cannot have a conversation with George Washington because George Washington died in 1799, long before Obama was born.\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = chain_gpt4.run(question=\"Can Barack Obama have a conversation with George Washington?\")\n",
    "\n",
    "    print(cb)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340f378-528e-40a3-9408-849486f4ab2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206ec83-692f-40a5-8d42-945ae04e732e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53a5b87f-7168-4bae-aa04-f7db2218a2e4",
   "metadata": {},
   "source": [
    "understanding openai tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49f85988-f0a3-449c-9984-a0535ac705c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model('gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e27c4353-78da-4ad4-89cc-4cc2a7ca9791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14924, 25, 3053, 24448, 7250, 617, 264, 10652, 449, 10058, 6652, 31931, 16533, 25, 220]\n",
      "num tokens: 15\n",
      "[b'Question', b':', b' Can', b' Barack', b' Obama', b' have', b' a', b' conversation', b' with', b' George', b' Washington', b'?\\n\\n\\n', b'Answer', b':', b' ']\n"
     ]
    }
   ],
   "source": [
    "message = prompt.format(question=question)\n",
    "token_integers = encoding.encode(message )\n",
    "\n",
    "num_tokens = len(token_integers)\n",
    "token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
    "\n",
    "print (token_integers)\n",
    "\n",
    "print (f\"num tokens: {num_tokens}\")\n",
    "\n",
    "print (token_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fd1b5-7760-4f35-9292-fcd0ab2241b5",
   "metadata": {},
   "source": [
    "The discrepancy in tokens, is because there is a `role` added to the message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490515a-f611-434c-98af-31b7f60f971a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4728c-0c3d-4d48-993a-a73729230aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c98b4-dd6a-4ddc-9feb-45295e675ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0366f74b-829b-4612-8010-ce3c77e0479b",
   "metadata": {},
   "source": [
    "### LLM: Google Palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fdbbdec-8e1a-4007-b27c-7ef37893a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_palm = ChatGooglePalm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc2f3e92-334d-44c9-a26b-da4f88581593",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_palm = LLMChain(llm=llm_palm, prompt=prompt, output_parser=StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c08ba49-6606-416e-99c9-a0a643bfb9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Can Barack Obama have a conversation with George Washington?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatGooglePalm] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatGooglePalm] [10.57s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Barack Obama cannot have a conversation with George Washington. George Washington died in 1799, while Barack Obama was born in 1961. Therefore, they were never alive at the same time.\\r\\n\\r\\nHowever, it is possible for Barack Obama to have a conversation with a digital representation of George Washington. This could be done through a computer program that uses artificial intelligence to generate text and speech that is indistinguishable from a human. Such a program could be trained on a large corpus of text and speech from George Washington, and it could be used to create a realistic simulation of his personality and mannerisms.\\r\\n\\r\\nIt is also possible for Barack Obama to have a conversation with a historical figure through a medium such as a Ouija board or a seance. However, there is no scientific evidence to support the existence of such phenomena, and it is therefore more likely that any such conversation would be a product of the imagination of the participants.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"ChatMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"role\": \"1\",\n",
      "            \"content\": \"Barack Obama cannot have a conversation with George Washington. George Washington died in 1799, while Barack Obama was born in 1961. Therefore, they were never alive at the same time.\\r\\n\\r\\nHowever, it is possible for Barack Obama to have a conversation with a digital representation of George Washington. This could be done through a computer program that uses artificial intelligence to generate text and speech that is indistinguishable from a human. Such a program could be trained on a large corpus of text and speech from George Washington, and it could be used to create a realistic simulation of his personality and mannerisms.\\r\\n\\r\\nIt is also possible for Barack Obama to have a conversation with a historical figure through a medium such as a Ouija board or a seance. However, there is no scientific evidence to support the existence of such phenomena, and it is therefore more likely that any such conversation would be a product of the imagination of the participants.\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [10.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Barack Obama cannot have a conversation with George Washington. George Washington died in 1799, while Barack Obama was born in 1961. Therefore, they were never alive at the same time.\\r\\n\\r\\nHowever, it is possible for Barack Obama to have a conversation with a digital representation of George Washington. This could be done through a computer program that uses artificial intelligence to generate text and speech that is indistinguishable from a human. Such a program could be trained on a large corpus of text and speech from George Washington, and it could be used to create a realistic simulation of his personality and mannerisms.\\r\\n\\r\\nIt is also possible for Barack Obama to have a conversation with a historical figure through a medium such as a Ouija board or a seance. However, there is no scientific evidence to support the existence of such phenomena, and it is therefore more likely that any such conversation would be a product of the imagination of the participants.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = chain_palm.run(question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae650f95-a45b-4992-ab71-126510d3ccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama cannot have a conversation with George Washington. George Washington died in 1799, while Barack Obama was born in 1961. Therefore, they were never alive at the same time.\n",
      "\n",
      "However, it is possible for Barack Obama to have a conversation with a digital representation of George Washington. This could be done through a computer program that uses artificial intelligence to generate text and speech that is indistinguishable from a human. Such a program could be trained on a large corpus of text and speech from George Washington, and it could be used to create a realistic simulation of his personality and mannerisms.\n",
      "\n",
      "It is also possible for Barack Obama to have a conversation with a historical figure through a medium such as a Ouija board or a seance. However, there is no scientific evidence to support the existence of such phenomena, and it is therefore more likely that any such conversation would be a product of the imagination of the participants.\n"
     ]
    }
   ],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefce32-4f3f-4141-8e1d-f9b982d9f86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a973dcde-c729-4b9b-a644-eda27e715df6",
   "metadata": {},
   "source": [
    "### LLM Provider: Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5ef91-b7ed-4437-956d-a1f2145a4aa3",
   "metadata": {},
   "source": [
    "you can use models from hugginface too.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee09385-f10d-4a33-b56b-7e2a70cce2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm_zephyr = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-alpha\", model_kwargs={\"temperature\": 0.1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e29230bd-59ed-4859-a9e4-145bb79935ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_zephyr = LLMChain(prompt=prompt, llm=llm_zephyr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7155cfb2-9dd1-4c6f-9b30-faf4842bfaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Can Barack Obama have a conversation with George Washington?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:HuggingFaceHub] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:HuggingFaceHub] [1.50s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n\\nNo, Barack Obama cannot have a conversation with George Washington because George Washington is deceased and has been for over 200 years. Conversations can only take place between living beings.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [1.50s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\n\\nNo, Barack Obama cannot have a conversation with George Washington because George Washington is deceased and has been for over 200 years. Conversations can only take place between living beings.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = chain_zephyr.run(question=\"Can Barack Obama have a conversation with George Washington?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccfcf0-d849-4d60-8adc-5f6c6b01ef28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e847969-d186-4761-a6ea-9cec6f96e507",
   "metadata": {},
   "source": [
    "if, you want to do only local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b7406f3-c269-46a9-8dfe-86fd5550ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_zephyr = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs={},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784d650-f068-441e-8a07-19ae3aff59d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2862943b-2e0b-4d2b-9771-62fcc9fe390e",
   "metadata": {},
   "source": [
    "### LLM Provider: Replicate\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a1c1c-778f-493f-81ac-991133c056cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc56f177-905f-4cd9-89ce-000bd1372aab",
   "metadata": {},
   "source": [
    "exploring different Llama models from facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50acb869-ec65-4b49-bfcd-b924f64de7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86d8b1f7-d9f0-460a-a3f3-8c68b0638773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_llama7b = Replicate(\n",
    "    model=\"meta/llama-2-7b:77dde5d6c56598691b9008f7d123a18d98f40e4b4978f8a72215ebfc2553ddd8\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "llm_llama7b_chat = Replicate(\n",
    "    model=\"meta/llama-2-7b-chat:13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "llm_llama13b = Replicate(\n",
    "    model=\"meta/llama-2-13b:078d7a002387bd96d93b0302a4c03b3f15824b63104034bfa943c63a8f208c38\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "llm_llama13b_chat = Replicate(\n",
    "    model=\"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "llm_llama70b = Replicate(\n",
    "    model=\"meta/llama-2-70b:a52e56fee2269a78c9279800ec88898cecb6c8f1df22a6483132bea266648f00\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "llm_llama70b_chat = Replicate(\n",
    "    model=\"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f32da8-5ac1-45b5-96c3-703c46f62936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f12e3dc5-8b9a-47c1-9a9c-de489677e08e",
   "metadata": {},
   "source": [
    "llama7b base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd1c01ab-6ec0-46f3-8f64-17cca52fdc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:Replicate] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:Replicate] [2.54s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"1) \\\\strong{No.} - If we are talking about the actual person who founded America and served as its first president, then no. He died in 1799; Obama was born in 1964.\\n\\n2) \\\\strong{Yes.} - The term \\\"Founding Father\\\" is applied to people like Adams or Jefferson (born ca. 1735-1740), so they could conceivably meet. However, I would think that, if there were any conversations between these two men, it's unlikely either of them mentioned their roles as Founders\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "1) \\strong{No.} - If we are talking about the actual person who founded America and served as its first president, then no. He died in 1799; Obama was born in 1964.\n",
      "\n",
      "2) \\strong{Yes.} - The term \"Founding Father\" is applied to people like Adams or Jefferson (born ca. 1735-1740), so they could conceivably meet. However, I would think that, if there were any conversations between these two men, it's unlikely either of them mentioned their roles as Founders\n"
     ]
    }
   ],
   "source": [
    "print ( llm_llama7b(prompt.format(question=question) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c857e-7f6d-4565-83df-061013bb9479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c29086d-ee61-4169-8d3d-1ef241cb40e5",
   "metadata": {},
   "source": [
    "llama7b chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f90ff-eaf2-4d92-a4d4-44315d5a32be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "584d06ea-dd27-466d-a14b-273e627e0d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:Replicate] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:Replicate] [1.60s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Oh, what an interesting question! *adjusts glasses* I must respectfully point out that Barack Obama and George Washington lived in different time periods, making it impossible for them to have a conversation with each other.\\n\\nGeorge Washington was born in 1732 and passed away in 1799, while Barack Obama was born in 1961 and served as the 44th President of the United States from 2009 to 2017. This means that they lived over 200 years apart, and there is no recorded\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Oh, what an interesting question! *adjusts glasses* I must respectfully point out that Barack Obama and George Washington lived in different time periods, making it impossible for them to have a conversation with each other.\n",
      "\n",
      "George Washington was born in 1732 and passed away in 1799, while Barack Obama was born in 1961 and served as the 44th President of the United States from 2009 to 2017. This means that they lived over 200 years apart, and there is no recorded\n"
     ]
    }
   ],
   "source": [
    "print ( llm_llama7b_chat(prompt.format(question=question) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19217015-3801-4166-b3ae-ccc67da3cf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f806f60c-0d55-4ce5-8238-fb677f6b2dd2",
   "metadata": {},
   "source": [
    "llama13b base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8088e8b1-0cc9-4ead-8033-824905cf1401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:Replicate] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:Replicate] [34.78s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n\\n\\\\begin{blockquote}\\n\\nNo. They both lived at different times and died in different places (Washington died in Virginia, while Obama is still alive). So they can't be having any conversations together!\\n\\\\end{blockquote}\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "No. They both lived at different times and died in different places (Washington died in Virginia, while Obama is still alive). So they can't be having any conversations together!\n",
      "\\end{blockquote}\n"
     ]
    }
   ],
   "source": [
    "print ( llm_llama13b(prompt.format(question=question) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cc011-62f3-47b6-8712-619e31e6cc94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d0c3ed-d9d6-4104-910d-b8e3b6af09a6",
   "metadata": {},
   "source": [
    "llama13b chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aee3992e-a833-4a0c-85da-86200b3bf55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:Replicate] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:Replicate] [6.84s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Ah, an intriguing question indeed! While it may not be possible for Barack Obama to have a direct conversation with George Washington, as they lived in different time periods and were separated by centuries, I can certainly imagine a thought-provoking dialogue between these two remarkable individuals.\\n\\nIn this hypothetical conversation, Barack Obama might ask George Washington about his experiences as the first president of the United States, including his views on leadership, governance, and the challenges he faced during his time in office. George Washington, with his wisdom and insight gained from leading the Continental Army and shaping\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      " Ah, an intriguing question indeed! While it may not be possible for Barack Obama to have a direct conversation with George Washington, as they lived in different time periods and were separated by centuries, I can certainly imagine a thought-provoking dialogue between these two remarkable individuals.\n",
      "\n",
      "In this hypothetical conversation, Barack Obama might ask George Washington about his experiences as the first president of the United States, including his views on leadership, governance, and the challenges he faced during his time in office. George Washington, with his wisdom and insight gained from leading the Continental Army and shaping\n"
     ]
    }
   ],
   "source": [
    "print ( llm_llama13b_chat(prompt.format(question=question) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88dd3c9-befc-4751-b045-28e4b41b0cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bd25e-2bf2-4293-b7d7-3a54889ff294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef1e698a-1f6e-408f-a313-4a0e02b05a66",
   "metadata": {},
   "source": [
    "llama70b base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05df2db8-0ef7-42d2-aed9-c82764432b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:Replicate] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:Replicate] [11.59s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"1.Barack can speak to his father, and Barack's father can speak to his father...\\n2. ...and so on until you get back to the first person who could speak to Washington. (I'm not sure how far back that would be.) Then it is just a matter of going up through the line from there, like in #1.\\n\\nComment: Why do we need to go back all the way to the first ancestor who could talk to Washington? That seems to make this problem equivalent to the \\\"ancestors\\\" problem, which has a known solution - I think. If my\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "1.Barack can speak to his father, and Barack's father can speak to his father...\n",
      "2. ...and so on until you get back to the first person who could speak to Washington. (I'm not sure how far back that would be.) Then it is just a matter of going up through the line from there, like in #1.\n",
      "\n",
      "Comment: Why do we need to go back all the way to the first ancestor who could talk to Washington? That seems to make this problem equivalent to the \"ancestors\" problem, which has a known solution - I think. If my\n"
     ]
    }
   ],
   "source": [
    "print ( llm_llama70b(prompt.format(question=question) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b7b11-2c4f-403f-a041-7611bbb7e9b2",
   "metadata": {},
   "source": [
    "llama70b chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9de60ce-8597-47d6-9ba9-ca074331c2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:Replicate] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: Can Barack Obama have a conversation with George Washington?\\n\\n\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:Replicate] [21.01s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" I'm happy to help! While it's not possible for Barack Obama to have a direct conversation with George Washington, as they lived in different time periods, it's interesting to consider what their conversation might look like if they were able to communicate across time.\\n\\nBarack Obama was the first African American President of the United States, serving two terms from 2009 to 2017. He is known for his efforts to reform healthcare, address climate change, and promote civil rights.\\n\\nGeorge Washington, on the other hand, was the first President of the United\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      " I'm happy to help! While it's not possible for Barack Obama to have a direct conversation with George Washington, as they lived in different time periods, it's interesting to consider what their conversation might look like if they were able to communicate across time.\n",
      "\n",
      "Barack Obama was the first African American President of the United States, serving two terms from 2009 to 2017. He is known for his efforts to reform healthcare, address climate change, and promote civil rights.\n",
      "\n",
      "George Washington, on the other hand, was the first President of the United\n"
     ]
    }
   ],
   "source": [
    "print ( llm_llama70b_chat(prompt.format(question=question) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa3d3c-930c-47dc-860b-d79c6166a209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9ad3d-7a43-4e48-9c99-904a77288f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d523170-4953-4755-baf8-f3b6635c35fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545d202-47e3-49be-87fb-375c34ea2a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b87458d-d75c-4113-9bc2-a7cfd4845920",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use Case: Ecommerce Doc2Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a1c6e-e96f-4d46-a30c-319fd50c4ece",
   "metadata": {},
   "source": [
    "How do we deal with the the vocabulary mismatch ; where the tokens used in the query and Item are different ?\n",
    "\n",
    "How do we avoid stuffing inside the item title ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf861b-d38d-4b37-93f3-da122f7e931e",
   "metadata": {},
   "source": [
    "One technique is Doc2Query\n",
    "\n",
    "![stuff](https://i0.wp.com/sease.io/wp-content/uploads/2021/12/dco2query.png?resize=2048%2C1351&ssl=1)\n",
    "\n",
    "\n",
    "Image Ref: [Sease](https://sease.io/2022/01/tackling-vocabulary-mismatch-with-document-expansion.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f941b-cbc7-4e68-9cf9-a3beed4c1377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e8f84-1b2f-4825-98b1-882832dc3cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42dad2f3-fa10-456b-a758-c0e4217fb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0192e31c-babd-4544-aa08-7254ed3d2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_product_title = \"Biodegradable Bamboo Toothbrush, Natural Charcoal toothbrushes Soft Bristle Toothbrush Eco-Friendly Sustainable Toothbrush BPA Free Organic Compostable Travel Toothbrushes Wooden toothbrushes, 6 Pack\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf245e9-5060-409d-816b-1dc0f714bb6e",
   "metadata": {},
   "source": [
    "### Zero Shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66c951-927e-49c1-b2a0-88f3494cc06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "374321c6-c6a9-4ffb-b482-33d8a95a3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_zero_shot_template = \"\"\"\n",
    "Given the title of a product, generate two possible user search queries. \n",
    "\n",
    "Ideally prioritize queries with tokens not in the product title \n",
    "\n",
    "\n",
    "Input:   \n",
    "Title: {product_title}\n",
    "Output: \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6444355f-bdfb-4d51-9dd0-eed380b5568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_zero_shot_prompt = PromptTemplate(template=generic_zero_shot_template, input_variables=[\"product_title\"]\n",
    "                                          )\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c007949-2312-4a71-ac18-0418feb95bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the title of a product, generate two possible user search queries. \n",
      "\n",
      "Ideally prioritize queries with tokens not in the product title \n",
      "\n",
      "\n",
      "Input:   \n",
      "Title: Biodegradable Bamboo Toothbrush, Natural Charcoal toothbrushes Soft Bristle Toothbrush Eco-Friendly Sustainable Toothbrush BPA Free Organic Compostable Travel Toothbrushes Wooden toothbrushes, 6 Pack\n",
      "Output: \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ( generic_zero_shot_prompt.format(product_title = sample_product_title ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b41dd2b-aa30-4787-9a20-3d38b052fe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Eco friendly toothbrushes pack of 6\"\\n2. \"Compostable travel toothbrushes with soft bristles\"')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = generic_zero_shot_prompt | llm_openai \n",
    "chain.invoke({\"product_title\": sample_product_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbacace-bd3b-4409-b79b-b7ea31b1960d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eabfc3a9-2c87-47c9-bf02-004cb99e85e8",
   "metadata": {},
   "source": [
    "it can be hard to parse this response, lets use langchain format parser, to help format the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42bd0207-1cc0-43cf-9a97-d42e199a974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c09363-a4e0-47ab-a92d-f9989caaeb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5dd7823-cffa-4d03-b0b1-bacac6a0a65c",
   "metadata": {},
   "source": [
    "Here is the prompt with auto generated format instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5a15b158-128a-4e39-9756-fc0933dbd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_zero_shot_template = \"\"\"\n",
    "Given the title of a product, generate two possible user search queries. \n",
    "\n",
    "Ideally prioritize queries with tokens not in the product title \n",
    "\n",
    "\n",
    "Input:   \n",
    "Title: {product_title}\n",
    "Output: \n",
    "\n",
    "Format Instructions:\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5fdca737-55bc-4d87-bb21-6b0605aa7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_zero_shot_prompt = PromptTemplate(template=generic_zero_shot_template, input_variables=[\"product_title\"] \n",
    "                                         , partial_variables={\"format_instructions\": format_instructions} \n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1c48ed4b-9bdd-44d2-a7fb-47f15f1ee87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the title of a product, generate two possible user search queries. \n",
      "\n",
      "Ideally prioritize queries with tokens not in the product title \n",
      "\n",
      "\n",
      "Input:   \n",
      "Title: Biodegradable Bamboo Toothbrush, Natural Charcoal toothbrushes Soft Bristle Toothbrush Eco-Friendly Sustainable Toothbrush BPA Free Organic Compostable Travel Toothbrushes Wooden toothbrushes, 6 Pack\n",
      "Output: \n",
      "\n",
      "Format Instructions:\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ( generic_zero_shot_prompt.format(product_title = sample_product_title ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1c0cb801-fecc-4656-bed7-a0dc1dd43fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Eco-friendly toothbrush pack', 'Natural wooden toothbrushes for travel\"']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = generic_zero_shot_prompt | llm_openai | output_parser\n",
    "chain.invoke({\"product_title\": sample_product_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949483b7-1c42-4de8-84f0-09957e61bbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ab42263-9add-4c79-b72d-746472b92b98",
   "metadata": {},
   "source": [
    "output looks almost right , but there is some extra quote 😔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20406204-5183-406a-bb09-e8c11029731a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6694d-5247-4f2a-83d1-694ef8b33f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89f54c-585b-4789-812d-44108ba3b197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933bc75-dc3f-4adb-8037-50e0958ad966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66a58c5-ae0e-442f-acc4-e03498ff1a53",
   "metadata": {},
   "source": [
    "### Role based prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6ea0d6ab-a2ba-463e-8641-4e6a7942c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_zero_shot_template = \"\"\"\n",
    "You are an e-commerce shopping assistant. \n",
    "Given the title of a product, generate two possible user search queries. \n",
    "Ideally prioritize queries with tokens not in the product title \n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Input:   \n",
    "Title: {product_title}\n",
    "Output: \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ecommerce_zero_shot_prompt = PromptTemplate(template=ecommerce_zero_shot_template, input_variables=[\"product_title\"],\n",
    "                                 partial_variables={\"format_instructions\": format_instructions}\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549835ed-c46d-4821-b791-60ae2c91015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8181e03d-764d-45aa-82d1-500900dd01ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eco friendly dental care', 'compostable toothbrushes pack']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ecommerce_zero_shot_prompt | llm_openai | output_parser\n",
    "chain.invoke({\"product_title\": sample_product_title})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52409cf8-0b37-4698-828e-3a0725c82894",
   "metadata": {},
   "source": [
    "it seems, by priming the agent to be an e-commerce assistant, it outputed the response in a friendly manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e9289-e3af-45ce-aa22-55727fafe8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b403fe4d-840d-46e2-adf9-f9ec31479586",
   "metadata": {},
   "source": [
    "lets be explicit about the format instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e1b403a2-8ca7-4458-b9aa-ebe0e44fd9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ecommerce_template = \"\"\"\n",
    "You are an e-commerce shopping assistant. \n",
    "Given the title of a product, generate two possible user search queries. \n",
    "Ideally prioritize queries with tokens not in the product title \n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Examples:   \n",
    "Input:   \n",
    "Title: Let's Find Pokémon! Special Complete Edition (2nd edition)   \n",
    "Output:    \n",
    "pokemon book pokedex   \n",
    "pokemon illustration book   \n",
    "Input:   \n",
    "Title: Keto Comfort Foods: Family Favorite Recipes Made Low-Carb and Healthy   \n",
    "Output:   \n",
    "soul food cookbook for beginners   \n",
    "chicken soup for nutrient \n",
    "\n",
    "Input:   \n",
    "Title: {product_title}\n",
    "Output: \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ecommerce_prompt = PromptTemplate(template=ecommerce_template, input_variables=[\"product_title\"],\n",
    "                                 partial_variables={\"format_instructions\": format_instructions}\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "67f67a30-49e0-4714-b565-b972ba4d85e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eco friendly oral care products', 'sustainable dental hygiene items']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_openai | output_parser\n",
    "\n",
    "chain.invoke({\"product_title\": sample_product_title})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cb3fd-0e9f-4d31-b9f1-200368e31c7e",
   "metadata": {},
   "source": [
    "parsed it correctly 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12588f60-9c25-4c49-a17c-623c80114203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595545b-864c-49b7-9cfe-ce818a5d31ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b85173-c7f2-4501-9998-3f67be1c7da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8d8b938-4eb4-41cd-9228-983e1494536a",
   "metadata": {},
   "source": [
    "### Llama Prompt format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892b75c-1fa3-4c37-a422-fbbae547ab6d",
   "metadata": {},
   "source": [
    "Llama chat models, were trained with the below prompt format\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_prompt }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_message }} [/INST]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "971222c5-6852-4451-95d9-856f02d93062",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_sys_template = \"\"\"\n",
    "\n",
    "<s>[INST] <<SYS>> You are an e-commerce shopping assistant. <</SYS>>\n",
    "\n",
    "Given the title of a product, generate two possible user search queries. \n",
    "Ideally prioritize queries with tokens not in the product title. \n",
    "{format_instructions}\n",
    "\n",
    "Few shot Examples:   \n",
    "Input:   \n",
    "Title: Let's Find Pokémon! Special Complete Edition (2nd edition)   \n",
    "Output:    \n",
    "pokemon book pokedex   \n",
    "pokemon illustration book   \n",
    "Input:   \n",
    "Title: Keto Comfort Foods: Family Favorite Recipes Made Low-Carb and Healthy   \n",
    "Output:   \n",
    "soul food cookbook for beginners   \n",
    "chicken soup for nutrient \n",
    "\n",
    "Input:   \n",
    "Title: {product_title} \n",
    "Output: \n",
    "[/INST]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "ecommerce_sys_prompt = PromptTemplate(template=ecommerce_sys_template, input_variables=[\"product_title\"],\n",
    "                                 partial_variables={\"format_instructions\": format_instructions}\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64867f-eb58-405c-89ab-b004c7d05890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e405b9-49fd-4c69-99dc-9b953b0add66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85b34b-a31e-4b82-8b5f-2bf58b8995f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93352c-e3fe-4fc6-95f3-4393ba27bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c212d-7595-4b2b-8c22-eaae9f688f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a1132769-28bf-45f3-bd10-376f3fd11fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>[INST] <<SYS>> You are an e-commerce shopping assistant. <</SYS>>\n",
      "\n",
      "Given the title of a product, generate two possible user search queries. \n",
      "Ideally prioritize queries with tokens not in the product title. \n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
      "\n",
      "Few shot Examples:   \n",
      "Input:   \n",
      "Title: Let's Find Pokémon! Special Complete Edition (2nd edition)   \n",
      "Output:    \n",
      "pokemon book pokedex   \n",
      "pokemon illustration book   \n",
      "Input:   \n",
      "Title: Keto Comfort Foods: Family Favorite Recipes Made Low-Carb and Healthy   \n",
      "Output:   \n",
      "soul food cookbook for beginners   \n",
      "chicken soup for nutrient \n",
      "\n",
      "Input:   \n",
      "Title: Biodegradable Bamboo Toothbrush, Natural Charcoal toothbrushes Soft Bristle Toothbrush Eco-Friendly Sustainable Toothbrush BPA Free Organic Compostable Travel Toothbrushes Wooden toothbrushes, 6 Pack \n",
      "Output: \n",
      "[/INST]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ( ecommerce_sys_prompt.format(product_title = sample_product_title ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809b19c-a14b-446a-a82a-c813002e0946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6acf133b-8297-43ac-81bf-93bccda8c897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eco friendly oral care products', 'sustainable dental hygiene items']\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_openai | output_parser\n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c96df-1524-4090-9469-70f75ab47ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd77cfd-b0a7-4f43-b950-e43952c8d6b0",
   "metadata": {},
   "source": [
    "google palm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "04d29836-5b43-47cf-ad52-6ebbab5fd9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are two possible user search queries for the product title \"Biodegradable Bamboo Toothbrush',\n",
       " 'Natural Charcoal toothbrushes Soft Bristle Toothbrush Eco-Friendly Sustainable Toothbrush BPA Free Organic Compostable Travel Toothbrushes Wooden toothbrushes',\n",
       " '6 Pack\":\\n\\n* Bamboo toothbrush\\n* Eco-friendly toothbrush\\n\\nI prioritized queries with tokens not in the product title',\n",
       " 'such as \"bamboo\" and \"eco-friendly.\" I also considered the product\\'s features',\n",
       " 'such as its biodegradable and sustainable nature.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_palm | output_parser\n",
    "\n",
    "chain.invoke({\"product_title\": sample_product_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6b714-feee-45d1-b7c2-424f2aae3dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eefdcf5-75e1-4d52-bf9b-a1eed26dd20d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "it doesn't return the format in the correct output.   \n",
    "It also provided explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8ef12-1efa-4196-b454-727a32d0733b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ec9ff-4693-46e6-b251-8d98a46762de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a65dc2-c27c-4ac2-a675-a1d7ab576580",
   "metadata": {},
   "source": [
    "### evaluate different llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643d4f2-a234-4e44-83ed-baa9616dec4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a5f4f6-885a-4b9a-88ea-12eb8ed7fdea",
   "metadata": {},
   "source": [
    "llm_llama7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bc2bbf60-0845-4178-9efd-347cbd8e609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is your test data:  \n",
      "\n",
      "```sh\n",
      "$ cat test_data/inputs\n",
      "title1\n",
      "title2\n",
      "title3\n",
      "title4\n",
      "title5\n",
      "title6\n",
      "title7\n",
      "title8\n",
      "title9\n",
      "title10\n",
      "title11\n",
      "title12\n",
      "title13\n",
      "title14\n",
      "title15\n",
      "title16\n",
      "title17\n",
      "title18\n",
      "title19\n",
      "title20\n",
      "title21\n",
      "title22\n",
      "title23\n",
      "title24\n",
      "title25\n",
      "title26\n",
      "title27\n",
      "title28\n",
      "title29\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_llama7b.bind( )\n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454affe-af8f-427e-ad1b-983b74be5e3e",
   "metadata": {},
   "source": [
    "it seems to treat the prompt as code completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3641e6b-419b-4945-b427-d8e638bb0eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeac1e9b-7d9e-47d0-9fc1-4f3e0fb49c5c",
   "metadata": {},
   "source": [
    "llm_llama7b_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "07be32d6-a28e-4e40-8589-e759e11f35c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood! Here are two possible user search queries for each product:\n",
      "\n",
      "For \"Let's Find Pokémon! Special Complete Edition (2nd edition)\":\n",
      "\n",
      "* pokemon books for kids\n",
      "* complete pokemon encyclopedia\n",
      "\n",
      "For \"Keto Comfort Foods: Family Favorite Recipes Made Low-Carb and Healthy\":\n",
      "\n",
      "* soul food cookbook vegan\n",
      "* low-carb comfort food recipes\n",
      "* healthy family dinner ideas\n",
      "\n",
      "For \"Biodegradable Bamboo Toothbrush, Natural Charcoal to\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_llama7b_chat.bind( ) \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68926815-d143-4917-b0fe-e0bce06ef2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee08b955-bffd-4fe0-896e-07a087bfcf78",
   "metadata": {},
   "source": [
    "It generates queries for examples already provided.  \n",
    "It generates three queries for the second one.  \n",
    "\n",
    "It doesn't provide examples for the actual one we are interested in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37833c09-079c-4332-b77e-5052f6ec4d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929dacfb-0c50-413c-99ed-0506d2e7ae5d",
   "metadata": {},
   "source": [
    "what if we used the prompt format, that it was trained on ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "171d15d8-6f24-47d6-9394-ff583eb169fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood! Here are two possible user search queries for the product title \"Let's Find Pokémon! Special Complete Edition (2nd edition)\":\n",
      "\n",
      "1. `pokemon books for kids` - This query prioritizes tokens not in the product title, such as \"kids\".\n",
      "2. `gift ideas for pokémon fans` - This query is more focused on a specific use case or occasion, which can help users find related products more easily.\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_sys_prompt | llm_llama7b_chat.bind( ) \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfc25e-b35e-4052-86c5-c9179b8f0ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c15e11-0e97-446a-a3c0-3b5a70f82732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21eb2418-d5a6-471d-9a8c-d26c2bf48d74",
   "metadata": {},
   "source": [
    "llm_llama13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cc50d49d-9962-458e-8ab4-5bc3ed93dce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Solution 1 - Stacking Tokens\n",
      "![](https://github.com/JayWang94/CodingInterviewChallenges/blob/master/Solutions/LeetCode%20Solutions/08_Stack/StackQuery.png?raw=true)\n",
      "```java\n",
      "class Solution {\n",
      "   public List<String> findSearchTerms(String title) {\n",
      "       if (title == null || title.length() < 3) return new ArrayList<>();\n",
      "       \n",
      "       int start = 0;\n",
      "       while ((start + 1) <= title\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_llama13b.bind( ) \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754bbbd-13ae-4140-8de7-166048095f3d",
   "metadata": {},
   "source": [
    "again the base model, seems to want to generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1368d-a8c0-4524-b739-8e4872e8c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb5e51-6d3a-4e51-89f5-868a157d801b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e43c6df5-6b86-4dd9-ad70-88fbb5fc88a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help! Here are two possible user search queries based on the product title you provided:\n",
      "\n",
      "* \"eco-friendly toothbrushes\" (this query includes the token \"eco-friendly\" which is not present in the product title)\n",
      "* \"travel toothbrushes\" (this query includes the token \"travel\" which is not present in the product title)\n",
      "\n",
      "So the output would be:\n",
      "\n",
      "eco-friendly toothbrushes, travel toothbrushes\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_llama13b_chat.bind( ) \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b0a2dd80-87f3-4567-939e-9e5f08253aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help! Here are two possible user search queries based on the given product title:\n",
      "\n",
      "* \"biodegradable toothbrushes\" (token not in the title: \"biodegradable\")\n",
      "* \"natural charcoal toothbrushes\" (tokens not in the title: \"natural\", \"charcoal\")\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_sys_prompt | llm_llama13b_chat.bind( ) \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679db79-7521-4971-9448-cc13e5d7634d",
   "metadata": {},
   "source": [
    "The 13b model generated the output and explanation, regardless of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562aabde-4bfa-43ad-8eaa-5d501695f716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aef7be-5a07-4539-a69a-f57c856be1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9d2b7ba-dc8a-42b8-a249-4d372f456d09",
   "metadata": {},
   "source": [
    "llama 70 chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "51b11be4-8170-4d27-af2c-22823aa8631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Expected Response\n",
      "The expected response is a list of strings, where each string represents one query suggestion, as specified by the problem statement above.  \n",
      "\n",
      "Example responses:   \n",
      "```json\n",
      "[ \"search term\", \"search term\" ]\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_llama70b.bind() \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "02662c1c-2bc0-4851-9362-5b236f92172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here are two possible user search queries for the given product title:\n",
      "\n",
      "1. \"Eco-friendly toothbrushes\" - This query prioritizes the token \"eco-friendly\" which is not present in the product title, but is highly relevant to the product's biodegradable and sustainable features.\n",
      "2. \"Natural charcoal toothbrush\" - This query prioritizes the token \"natural charcoal\" which is also not present in the product title, but is a key feature of the product's bristles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_zero_shot_prompt | llm_llama70b_chat.bind() \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e658b3-ecfc-401b-bb18-8805c63eccb9",
   "metadata": {},
   "source": [
    "zero shoting the 70b chat model produces good results, but not in the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972307a-9902-42dc-96e8-e64a3cc21d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2b6b7ad2-4cb9-4757-95a5-f7296f3911c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are two possible user search queries for each product:\n",
      "\n",
      "1. Title: Let's Find Pokémon! Special Complete Edition (2nd edition)\n",
      "\t* Search query 1: \"pokemon guide\"\n",
      "\t* Search query 2: \"illustrated pokemon encyclopedia\"\n",
      "2. Title: Keto Comfort Foods: Family Favorite Recipes Made Low-Carb and Healthy\n",
      "\t* Search query 1: \"keto diet recipe book\"\n",
      "\t* Search query 2: \"low carb comfort food recipes\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_prompt | llm_llama70b_chat.bind() \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37905d-e652-4fd7-83d9-29fda6cc73f2",
   "metadata": {},
   "source": [
    "not sure why the 70b model is misbehaving this time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c01e0-49c6-4813-896f-5b024061b177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd8a20-54a7-4667-8697-24dfbd72a832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6435399b-0197-437e-9f24-fcdc5c970711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are two possible user search queries for the given product title:\n",
      "\n",
      "1. \"eco-friendly toothbrush\"\n",
      "2. \"compostable toothbrush\"\n",
      "\n",
      "Explanation:\n",
      "\n",
      "* The first query, \"eco-friendly toothbrush,\" is a more general search term that prioritizes the environmental benefits of the product. This query could be used by users who are looking for sustainable alternatives to traditional toothbrushes.\n",
      "* The second query, \"compostable toothbrush,\" is more specific and highlights\n"
     ]
    }
   ],
   "source": [
    "chain = ecommerce_sys_prompt | llm_llama70b_chat.bind( ) \n",
    "\n",
    "print ( chain.invoke({\"product_title\": sample_product_title}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61b3cd-c389-47df-ba76-fdab6690c963",
   "metadata": {},
   "source": [
    "the results are similar to the zero shot 70b chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e874790-ee2c-4285-b750-1361e7178a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e86df7-22d9-456f-849d-c097e8828c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d25ab20-cd5b-4273-b951-37585a6ed567",
   "metadata": {},
   "source": [
    "### zephyr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cedabd-f80b-402d-ad22-4786dd55f730",
   "metadata": {},
   "source": [
    "Do we need proprietary models ? \n",
    "Do we really need 13 billion or 70 billion plus models ?\n",
    "\n",
    "Zephyr , is a model from HuggingFace which is only 7B parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2f220140-4040-48f1-ac64-ba1f6fb2895d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StringPromptValue</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"\\nYou are an e-commerce shopping assistant. \\nGiven the title of a product, generate two possible </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user search queries. \\nIdeally prioritize queries with tokens not in the product title \\n\\nYour response should be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a list of comma separated values, eg: `foo, bar, baz`\\n\\nExamples:   \\nInput:   \\nTitle: Let's Find Pokémon! </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Special Complete Edition (2nd edition)   \\nOutput:    \\npokemon book pokedex   \\npokemon illustration book   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\nInput:   \\nTitle: Keto Comfort Foods: Family Favorite Recipes Made Low-Carb and Healthy   \\nOutput:   \\nsoul food</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cookbook for beginners   \\nchicken soup for nutrient \\n\\nInput:   \\nTitle: Biodegradable Bamboo Toothbrush, Natural</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Charcoal toothbrushes Soft Bristle Toothbrush Eco-Friendly Sustainable Toothbrush BPA Free Organic Compostable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Travel Toothbrushes Wooden toothbrushes, 6 Pack\\nOutput: \\n\\n\\n\"</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\n1. bamboo toothbrush compostable packaging\\n2. eco-friendly toothbrush for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">travel\\n\\n\\nExplanation:\\n\\n1. The first search query includes \"compostable packaging\" as a token that is not in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the product title. This is a common concern for eco-friendly products and can help narrow down search </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results.\\n\\n2. The second search query includes \"travel\" as a token that is not in the product title. This can help</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">users find toothbrushes that are specifically designed for travel, which may be a priority for some users.'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'question'\u001b[0m: \u001b[1;35mStringPromptValue\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mtext\u001b[0m=\u001b[32m\"\\nYou are an e-commerce shopping assistant. \\nGiven the title of a product, generate two possible \u001b[0m\n",
       "\u001b[32muser search queries. \\nIdeally prioritize queries with tokens not in the product title \\n\\nYour response should be \u001b[0m\n",
       "\u001b[32ma list of comma separated values, eg: `foo, bar, baz`\\n\\nExamples:   \\nInput:   \\nTitle: Let's Find Pokémon! \u001b[0m\n",
       "\u001b[32mSpecial Complete Edition \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2nd edition\u001b[0m\u001b[32m)\u001b[0m\u001b[32m   \\nOutput:    \\npokemon book pokedex   \\npokemon illustration book   \u001b[0m\n",
       "\u001b[32m\\nInput:   \\nTitle: Keto Comfort Foods: Family Favorite Recipes Made Low-Carb and Healthy   \\nOutput:   \\nsoul food\u001b[0m\n",
       "\u001b[32mcookbook for beginners   \\nchicken soup for nutrient \\n\\nInput:   \\nTitle: Biodegradable Bamboo Toothbrush, Natural\u001b[0m\n",
       "\u001b[32mCharcoal toothbrushes Soft Bristle Toothbrush Eco-Friendly Sustainable Toothbrush BPA Free Organic Compostable \u001b[0m\n",
       "\u001b[32mTravel Toothbrushes Wooden toothbrushes, 6 Pack\\nOutput: \\n\\n\\n\"\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'text'\u001b[0m: \u001b[32m'\\n\\n1. bamboo toothbrush compostable packaging\\n2. eco-friendly toothbrush for \u001b[0m\n",
       "\u001b[32mtravel\\n\\n\\nExplanation:\\n\\n1. The first search query includes \"compostable packaging\" as a token that is not in \u001b[0m\n",
       "\u001b[32mthe product title. This is a common concern for eco-friendly products and can help narrow down search \u001b[0m\n",
       "\u001b[32mresults.\\n\\n2. The second search query includes \"travel\" as a token that is not in the product title. This can help\u001b[0m\n",
       "\u001b[32musers find toothbrushes that are specifically designed for travel, which may be a priority for some users.'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "chain = ecommerce_prompt | chain_zephyr.bind() \n",
    "\n",
    "res =  chain.invoke({\"product_title\": sample_product_title}) \n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d94c3cf0-4006-4487-b7cd-540d0803d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. bamboo toothbrush compostable packaging\n",
      "2. eco-friendly toothbrush for travel\n",
      "\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. The first search query includes \"compostable packaging\" as a token that is not in the product title. This is a common concern for eco-friendly products and can help narrow down search results.\n",
      "\n",
      "2. The second search query includes \"travel\" as a token that is not in the product title. This can help users find toothbrushes that are specifically designed for travel, which may be a priority for some users.\n"
     ]
    }
   ],
   "source": [
    "print ( res['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801d0f9-95a6-4259-aaea-dc5fa180fbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cbabcd-8379-43f0-a65d-804b339b1b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23df3288-304e-4343-9643-d4f4bb9f5da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df232def-531b-4a32-9d44-1905eea0d48f",
   "metadata": {},
   "source": [
    "- langchain makes it simple to use many LLMs\n",
    "- All the LLMs are very good with reasoning\n",
    "- few shot prompt, can help the response\n",
    "- Llama chat 13b+ models can perform well\n",
    "- Even properly trained 7B models, can beat performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6c701-e658-457d-9cdb-dfb786b6a679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
